{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7V_5ld8PuRT9"
      },
      "source": [
        "# <center><b>Assignment 2</b></center>\n",
        "### Name: Anshul Rustogi\n",
        "#### Roll No: 2010110111\n",
        "### Name: Kolluru jeshwanth\n",
        "#### Roll No: 2010110351\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G9HNdZluRUC"
      },
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wek9Pkh5uRUD",
        "outputId": "19fc419d-de17-4c33-904a-4aeb21f8396e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk # Natural Language Toolkit\n",
        "import re # Regular Expression\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU3W0jdzxCR2",
        "outputId": "63047951-56d6-43de-f51a-3d67a1583599"
      },
      "outputs": [],
      "source": [
        "# # connecting to google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_kgS71BuRUF"
      },
      "source": [
        "### <b>Question:</b>Vector Space Model\n",
        "In this assignment, you will be implementing ranked retrieval using vector space model. To implement the VSM, you may choose to implement your dictionary and postings lists in the following format. The only difference between this format and that in the textbook, is that you encode term frequencies in the postings for the purpose of computing tf×idf. The tuple in each posting represents (doc ID, term freq)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5b0W6ZauRUF"
      },
      "source": [
        "#### Step 1: Reading the data\n",
        "##### Note: Data taken from last IR assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "22jX_bjSuRUG",
        "outputId": "ce54ff66-e230-4a76-f4cf-7e16958ad455"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>levis.txt</td>\n",
              "      <td>what is levis?\\n\\nWalter A. Haas, (born May 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nike.txt</td>\n",
              "      <td>What is nike?\\n\\nNike is a champion brand buil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dell.txt</td>\n",
              "      <td>What is dell?\\n\\nThe company, first named PC’s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>huawei.txt</td>\n",
              "      <td>what is huawei?\\n\\nHuawei Technologies Co., Lt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ola.txt</td>\n",
              "      <td>What is Ola?\\n\\nOla needs no introduction. The...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Title                                            Content\n",
              "0   levis.txt  what is levis?\\n\\nWalter A. Haas, (born May 11...\n",
              "1    nike.txt  What is nike?\\n\\nNike is a champion brand buil...\n",
              "2    Dell.txt  What is dell?\\n\\nThe company, first named PC’s...\n",
              "3  huawei.txt  what is huawei?\\n\\nHuawei Technologies Co., Lt...\n",
              "4     Ola.txt  What is Ola?\\n\\nOla needs no introduction. The..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "df = pd.DataFrame(columns=['Title','Content'])\n",
        "# for dirname, _, filenames in os.walk('/content/drive/MyDrive/Corpus'):\n",
        "for dirname, _, filenames in os.walk('Corpus'):\n",
        "    for filename in filenames:\n",
        "        #print(os.path.join(dirname, filename))\n",
        "        with open(os.path.join(dirname, filename), 'r') as f:\n",
        "            df = pd.concat([df, pd.DataFrame({'Title':filename, 'Content':f.read()}, index=[0])], ignore_index=True)\n",
        "\n",
        "def retrieve_titles(indexes):\n",
        "    titles = []\n",
        "    for index in indexes:\n",
        "        titles.append(df['Title'][index])\n",
        "    return titles\n",
        "    \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcQ3tQzzuRUH"
      },
      "source": [
        "#### Step 2: Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "MwryupaeuRUH",
        "outputId": "006d0c27-6088-4726-bbe5-17c7c8280477"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Content</th>\n",
              "      <th>Content_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>levis.txt</td>\n",
              "      <td>what is levis?\\n\\nWalter A. Haas, (born May 11...</td>\n",
              "      <td>[levi, walter, haa, born, may, san, francisco,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nike.txt</td>\n",
              "      <td>What is nike?\\n\\nNike is a champion brand buil...</td>\n",
              "      <td>[nike, nike, champion, brand, builder, adverti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dell.txt</td>\n",
              "      <td>What is dell?\\n\\nThe company, first named PC’s...</td>\n",
              "      <td>[dell, compani, first, name, pc, limit, found,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>huawei.txt</td>\n",
              "      <td>what is huawei?\\n\\nHuawei Technologies Co., Lt...</td>\n",
              "      <td>[huawei, huawei, technolog, co, ltd, chines, m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ola.txt</td>\n",
              "      <td>What is Ola?\\n\\nOla needs no introduction. The...</td>\n",
              "      <td>[ola, ola, need, introduct, first, indian, cab...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Title                                            Content  \\\n",
              "0   levis.txt  what is levis?\\n\\nWalter A. Haas, (born May 11...   \n",
              "1    nike.txt  What is nike?\\n\\nNike is a champion brand buil...   \n",
              "2    Dell.txt  What is dell?\\n\\nThe company, first named PC’s...   \n",
              "3  huawei.txt  what is huawei?\\n\\nHuawei Technologies Co., Lt...   \n",
              "4     Ola.txt  What is Ola?\\n\\nOla needs no introduction. The...   \n",
              "\n",
              "                                Content_preprocessed  \n",
              "0  [levi, walter, haa, born, may, san, francisco,...  \n",
              "1  [nike, nike, champion, brand, builder, adverti...  \n",
              "2  [dell, compani, first, name, pc, limit, found,...  \n",
              "3  [huawei, huawei, technolog, co, ltd, chines, m...  \n",
              "4  [ola, ola, need, introduct, first, indian, cab...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Creating a function to do the preprocessing: case folding, tokenization, stopword removal, lemmatization and stemming\n",
        "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "  \n",
        "    #Case Folding: convert all characters to lowercase\n",
        "    text = text.lower()\n",
        "    #Remove punctuation\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "\n",
        "    #Tokenization: using own tokenizer\n",
        "    text = text.strip()\n",
        "    tokens = re.split('\\W+', text)\n",
        "\n",
        "    #Remove empty tokens and single character tokens\n",
        "    tokens = [token for token in tokens if token != '' and len(token) > 1]\n",
        "\n",
        "    #Stopword Removal: remove stopwords like 'a', 'the', 'is', etc.\n",
        "    tokens = [token for token in tokens if token not in nltk_stopwords]\n",
        "    #Lemmatization: convert words to their base form (e.g. 'better' to 'good')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    #Stemming: convert words to their root form (e.g. 'caring' to 'care', 'launched' to 'launch', etc.)\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "#Applying the function to the dataframe\n",
        "df['Content_preprocessed'] = df['Content'].apply(preprocess)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmL3BxmnuRUI"
      },
      "source": [
        "#### Step 3: Creating the tf x idf\n",
        "In the searching step, you will need to rank documents by cosine similarity based on tf×idf. In\n",
        "terms of SMART notation of ddd.qqq, you will need to implement the lnc.ltc ranking scheme (i.e., log tf and idf with cosine normalization for queries documents, and log tf, cosine normalization but no idf for documents. Compute cosine similarity between the query and each document, with the weights follow the tf×idf calculation, where term freq = 1 + log(tf) and inverse document frequency idf = log(N/df) (for queries). That is,\n",
        "tf-idf = (1 + log(tf)) * log(N/df).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rf2vYCjuRUJ",
        "outputId": "ab420651-6737-4161-ad2f-b56e8bf3c027"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The given question asks us to apply the vector space model to the corpus with different weighting schemes: \n",
        "1. For document:\n",
        "    a. For term frequency: logaritmic\n",
        "    b. For document frequency: none\n",
        "    c. Normalization: cosine\n",
        "\n",
        "2. For quey:\n",
        "    a. For term frequency: logaritmic\n",
        "    b. For document frequency: t ie inverse document frequency\n",
        "    c. Normalization: cosine\n",
        "'''\n",
        "\n",
        "#Defining some basic functions for the vector space model\n",
        "def calculate_idf(N: int, df: int) -> float:\n",
        "    '''\n",
        "    calculate the inverse document frequency (idf) of a term\n",
        "    :param N: number of documents\n",
        "    :param df: document frequency of a term (number of documents containing the term)\n",
        "    '''\n",
        "    return math.log(N/df, 10)\n",
        "\n",
        "def get_document_frequency(term: str, documents: list) -> int:\n",
        "    '''\n",
        "    calculate the document frequency of a term in a list of documents\n",
        "    :param term: the term\n",
        "    :param documents: the list of documents\n",
        "    '''\n",
        "    count = 0\n",
        "    for document in documents:\n",
        "        if term in document:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def log_term_frequency(freq: int) -> float:\n",
        "    '''\n",
        "    calculate the log term frequency of a term in a document\n",
        "    :param freq: the term frequency\n",
        "    '''\n",
        "    if freq == 0:\n",
        "        return 0\n",
        "    return 1 + math.log(freq, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_documents(df):\n",
        "    document_vectors = []\n",
        "    for document in df['Content_preprocessed']:\n",
        "        #Calculating the term frequency\n",
        "        term_frequency = {}\n",
        "        for term in document:\n",
        "            if term in term_frequency:\n",
        "                term_frequency[term] += 1\n",
        "            else:\n",
        "                term_frequency[term] = 1\n",
        "        #Calculating the log term frequency\n",
        "        for term in term_frequency:\n",
        "            term_frequency[term] = log_term_frequency(term_frequency[term])\n",
        "        #Taking cosine normalization\n",
        "        sum_of_squares = sum([freq**2 for freq in term_frequency.values()])\n",
        "        for term in term_frequency:\n",
        "            term_frequency[term] /= math.sqrt(sum_of_squares)\n",
        "            \n",
        "        document_vectors.append(term_frequency)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "dc_vectors = preprocess_documents(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_query(query: str) -> list:\n",
        "    '''\n",
        "    preprocess the query\n",
        "    :param query: the query\n",
        "    '''\n",
        "\n",
        "    #Preprocessing the query\n",
        "    query = preprocess(query)\n",
        "    #Calculating the term frequency of the query\n",
        "    query_vector = {}\n",
        "    for term in query:\n",
        "        if term in query_vector:\n",
        "            query_vector[term] += 1\n",
        "        else:\n",
        "            query_vector[term] = 1\n",
        "    #Calculating the inverse document frequency of the query\n",
        "    idf = {}\n",
        "    for term in query_vector:\n",
        "        idf[term] = calculate_idf(len(df), get_document_frequency(term, df['Content_preprocessed']))\n",
        "    #Calculating the log term frequency of the query\n",
        "    for term in query_vector:\n",
        "        query_vector[term] = log_term_frequency(query_vector[term])\n",
        "    #Multiplying the term frequency with the inverse document frequency\n",
        "    for term in query_vector:\n",
        "        query_vector[term] = query_vector[term] * idf[term]\n",
        "\n",
        "    #Normalizing the query vector with cosine normalization\n",
        "    sum_of_squares = sum([freq**2 for freq in query_vector.values()])\n",
        "    for term in query_vector:\n",
        "        query_vector[term] /= math.sqrt(sum_of_squares)\n",
        "\n",
        "    return query_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(query_vector: dict, document_vector: dict) -> float:\n",
        "    '''\n",
        "    calculate the cosine similarity between a query and a document\n",
        "    :param query_vector: the query vector\n",
        "    :param document_vector: the document vector\n",
        "    '''\n",
        "    numerator = 0\n",
        "    for term in query_vector:\n",
        "        if term in document_vector:\n",
        "            numerator += query_vector[term] * document_vector[term]\n",
        "    return numerator\n",
        "\n",
        "def get_top_k_documents(query: str, k: int) -> list:\n",
        "    '''\n",
        "    get the top k documents for a query\n",
        "    :param query: the query\n",
        "    :param k: number of documents to return\n",
        "    '''\n",
        "    query_vector = preprocess_query(query)\n",
        "    scores = []\n",
        "    for document_vector in dc_vectors:\n",
        "        scores.append(calculate_cosine_similarity(query_vector, document_vector))\n",
        "    #print(scores)\n",
        "    #Printing the top k documents with their titles and scores\n",
        "    #Getting the indices of the top k documents\n",
        "    top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    for i in range(k):\n",
        "        print('\\n' + str(i+1) + ')\\nTitle: ', df.iloc[top_k_indices[i]]['Title'])\n",
        "        print('Score: ', scores[top_k_indices[i]], end=\"\\n---------------XXX---------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1)\n",
            "Title:  shakespeare.txt\n",
            "Score:  0.12296257967131181\n",
            "---------------XXX---------------\n",
            "2)\n",
            "Title:  levis.txt\n",
            "Score:  0.025503933795216413\n",
            "---------------XXX---------------\n",
            "3)\n",
            "Title:  Adobe.txt\n",
            "Score:  0.023453516952090917\n",
            "---------------XXX---------------\n",
            "4)\n",
            "Title:  google.txt\n",
            "Score:  0.021295839437710868\n",
            "---------------XXX---------------\n",
            "5)\n",
            "Title:  nike.txt\n",
            "Score:  0.019556033706669237\n",
            "---------------XXX---------------"
          ]
        }
      ],
      "source": [
        "get_top_k_documents('Warwickshire, came from an ancient family and was the heiress to some land', 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8bce32cf944d4feb3a22d4f60d9fa78bd442b8495f83c71c09932da75801ae75"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
